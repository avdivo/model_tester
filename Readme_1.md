# Система для тестирования и оценки LLM моделей

Это приложение представляет собой инструмент для проведения тестирования и оценки производительности различных больших языковых моделей (LLM). Оно позволяет автоматизировать процесс отправки запросов к моделям, сравнивать их ответы с эталонными и анализировать результаты по нескольким ключевым метрикам.

## Ключевые возможности

*   **Гибкая настройка тестов:** Создавайте собственные наборы тестов в простых Markdown-файлах, определяя системные роли, промпты, а также наборы вопросов и ожидаемых ответов.
*   **Поддержка множества моделей:** Легко переключайтесь между любыми моделями, доступными через API OpenRouter.
*   **Автоматическая оценка:** Система автоматически сравнивает ответы модели с эталонными. Поддерживается как прямое сравнение JSON-объектов, так и оценка корректности текстовых ответов с помощью LLM.
*   **Комплексный анализ:** Для каждого теста собирается подробная статистика, включая:
    *   Время ответа для каждого запроса.
    *   Количество использованных токенов (на вход и на выход).
    *   Стоимость каждого запроса и всего теста.
    *   Процент правильных ответов.
*   **Система баллов:** Рассчитывается итоговый балл производительности модели, который учитывает как точность ответов, так и медианное время задержки.
*   **Детальные отчеты:** Результаты каждого теста сохраняются в нескольких форматах:
    *   Подробный лог в текстовом файле.
    *   Сводная информация в консоли в наглядном табличном виде.
    *   Агрегированные данные добавляются в общую Excel-таблицу для дальнейшего анализа.

## Как это работает

Весь процесс можно разделить на несколько этапов:

1.  **Конфигурация:** В главном файле `main.py` вы указываете, какую модель и какой набор тестов использовать.
2.  **Чтение теста:** Скрипт читает Markdown-файл из папки `tests/`, извлекая из него описание, системную роль, основной промпт и список вопросов с ответами.
3.  **Выполнение запросов:** Приложение итерируется по вопросам, отправляя асинхронные запросы к указанной модели через API OpenRouter.
4.  **Анализ ответов:** Каждый ответ модели сравнивается с эталонным из файла теста.
5.  **Расчет метрик и баллов:** После завершения всех запросов рассчитываются общие показатели: стоимость, использование токенов, процент правильных ответов и итоговый балл.
6.  **Сохранение результатов:** Вся информация выводится в консоль и сохраняется в файлы отчетов в папках `result/` и `report/`.

## Быстрый старт

### Требования

*   Python 3.x
*   Ключ API от [OpenRouter](https://openrouter.ai/keys)

### Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL репозитория>
    cd <папка проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install aiohttp python-dotenv openpyxl tabulate
    ```

4.  **Настройте API ключ:**
    *   Создайте файл `.env` в корневой папке проекта.
    *   Добавьте в него ваш ключ API в следующем формате:
        ```
        OPENROUTER_API_KEY="ваш_ключ_здесь"
        ```

### Запуск тестов

1.  **Выберите тест и модель:**
    *   Откройте файл `main.py`.
    *   Установите значение переменной `model` (например, `"google/gemini-pro"`).
    *   Установите значение переменной `test_name` (имя файла из папки `tests` без расширения `.md`, например, `"get_metadata_1"`).

2.  **Запустите скрипт:**
    ```bash
    python3 main.py
    ```

Результаты будут выведены в консоль, а также сохранены в соответствующие файлы отчетов.

## Структура теста

Тестовые сценарии находятся в папке `tests/` и имеют формат Markdown. Каждый файл должен содержать следующие секции, разделенные заголовками:

*   `# Описание`: Краткое описание цели теста.
*   `# Роль`: Системный промпт, определяющий поведение модели.
*   `# Промпт`: Основной промпт, который будет использоваться для всех вопросов в тесте.
*   `# Тесты`: Секция, содержащая пары вопросов и ответов.
    *   `## Вопрос X`: Текст вопроса к модели.
    *   `## Ответ X`: Эталонный ответ. Если ответ представлен в формате JSON, система попытается сравнить его как словарь. В противном случае будет выполнена оценка текста.

## Отчетность и оценка

### Форматы отчетов

*   **Консоль:** Таблицы с основной информацией о тесте и результатами по каждому вопросу.
*   **Текстовые файлы (`result/`):** Детальный лог всего процесса тестирования, включая полные тексты вопросов, ответов модели и эталонных ответов. Имя файла соответствует имени модели.
*   **Excel (`report/report.xlsx`):** Сводная таблица, куда после каждого теста добавляется новая строка с ключевыми метриками: модель, название теста, медианная задержка, процент правильных ответов, итоговый балл и стоимость.

### Расчет баллов

Итоговый балл (`score`) является интегральной оценкой производительности модели и рассчитывается в `report/calc_ball.py`. Формула учитывает два основных фактора:

1.  **Точность (`accuracy`):** Процент правильных ответов является базовым значением для оценки.
2.  **Скорость (`median_time`):** Из базовых баллов вычитается штраф за медлительность. Штраф рассчитывается пропорционально тому, насколько медианное время ответа превышает установленный минимальный порог (`t_min`).

Это позволяет сбалансированно оценить модели, находя компромисс между качеством и скоростью ответов.

## Структура проекта

```
/
├─── main.py               # Главный скрипт для запуска тестов
├─── func.py               # Вспомогательные функции (парсер, запись в файл)
├─── .env                  # Конфигурация с API ключами
├─── providers/            # Модули для работы с API поставщиков моделей
│    ├─── open_router_async.py
│    └─── open_router.py
├─── report/               # Модули для генерации отчетов
│    ├─── calc_ball.py     # Логика расчета итогового балла
│    ├─── check.py         # Функции для сверки ответов
│    ├─── to_excel.py      # Запись результатов в Excel
│    └─── report.xlsx      # Итоговый отчет в формате Excel
├─── result/               # Папка для сохранения детальных текстовых логов
├─── tests/                # Наборы тестов в формате Markdown
├─── Readme.md             # Оригинальное описание проекта
└─── Readme_1.md           # Расширенное описание проекта (этот файл)
```

*   **`main.py`**: Главный исполняемый файл. Здесь вы настраиваете, какую модель и какой тест запускать, а также можете изменить параметры генерации (temperature, max_tokens и т.д.).
*   **`func.py`**: Содержит вспомогательные функции, используемые в основной программе, такие как извлечение секций из Markdown-файлов и запись текстовых отчетов.
*   **`providers/`**: Директория для интеграции с различными API LLM. По умолчанию используется OpenRouter.
*   **`report/`**: Содержит всю логику, связанную с анализом и представлением результатов.
*   **`result/`**: В эту папку сохраняются подробные текстовые отчеты о каждом запуске теста. Имя файла формируется из названия модели.
*   **`tests/`**: Здесь хранятся ваши тестовые сценарии. Вы можете создавать новые `.md` файлы для проверки различных гипотез и задач.

