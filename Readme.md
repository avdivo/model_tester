# Система для тестирования и оценки LLM моделей

Это приложение представляет собой гибкий и мощный инструмент для проведения тестирования и оценки производительности различных больших языковых моделей (LLM). 

С его помощью вы можете автоматизировать проверку качества ответов моделей, сравнить их между собой по скорости, точности и стоимости, а также найти оптимальные параметры для решения ваших задач.

## Быстрый старт

Этот раздел поможет вам запустить проект и получить первые результаты за 3 шага.

1.  **Настройте API-ключ.** Создайте в корне проекта файл `.env` и добавьте в него ваш ключ от [OpenRouter](https://openrouter.ai/keys):
    ```
    OPENROUTER_API_KEY="ваш_ключ_здесь"
    ```

2.  **Установите зависимости.** Выполните в терминале:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Запустите тесты.** В проекте уже есть готовый демонстрационный набор. Просто запустите главный скрипт:
    ```bash
    python3 main.py
    ```

Готово! Вы увидите в консоли ход выполнения тестов.

## Что вы получите: Форматы отчетов

После выполнения скрипта вы получите результаты в трех форматах:

1.  **Вывод в консоли:** Наглядные таблицы с прогрессом и итоговой статистикой по каждому тесту.
2.  **Детальный лог (`result/`):** В эту папку для каждой модели создается текстовый файл с полной историей диалога: вопросы, эталонные ответы, ответы модели и вердикт (ВЕРНО/ОШИБКА).
3.  **Сводный отчет (`report/report.xlsx`):** В эту Excel-таблицу после каждого запуска добавляется строка с ключевыми метриками: модель, тест, медианное время ответа, процент правильных ответов, итоговый балл и стоимость.

## Как это работает: Подробное руководство и примеры

Система имеет гибкую структуру, которую вы можете настраивать для решения своих задач.

### Ключевые концепции

1.  **Набор тестов (`Test Suite`)**: Определяет, *какие* модели, *на каких* тестах и с *какими* параметрами будут запущены. Задается в `test_suites.md`.
2.  **Тест (`Test`)**: Сценарий проверки в `.md` файле (`tests/`). Содержит промпты, вопросы, ответы и **правила оценки**.
3.  **Конфигурация (`Config`)**: Параметры для API модели (`temperature`, `max_tokens` и т.д.) в `.json` файле (`configs/`).

Ниже приведены примеры каждого из этих файлов с разбором.

---

### Пример 1: Файл конфигурации (`configs/standard.json`)

В проекте уже есть готовые файлы `standard.json` и `full.json`. Вам не нужно создавать их с нуля, вы можете использовать их или скопировать и изменить под свои нужды.

```json
{
  "param": {
    "temperature": 0.5,
    "max_tokens": 2048
  },
  "response_format": {
    "type": "json_object"
  },
  "extra_body": null
}
```
*   `param`: Основные параметры, которые передаются в API модели.
    *   `temperature`: Контролирует "креативность" или случайность ответов. `0.5` — сбалансированный вариант.
    *   `max_tokens`: Максимальное количество токенов в ответе.
*   `response_format`: Указывает модели, что ответ должен быть в формате JSON. Крайне полезно для тестов на извлечение данных.
*   `extra_body`: Дополнительные, реже используемые параметры, специфичные для некоторых провайдеров.

---

### Пример 2: Файл теста (`tests/test_model_check.md`)

Это пример теста на проверку общих знаний с гибкой оценкой.

```markdown
# Описание
Проверка проверки ответа модели другой моделью.

# Роль
Ты — консультант.

# Промпт
Ответь на вопрос коротко и только по делу.
Ответь текстом без кавычек.

# Настройки
## Сравнение ответа модели текстом
Модель

# Тесты
## Вопрос 1
кто открыл Америку?
## Ответ 1
Колумб
```
*   `# Описание`: Краткая цель этого теста.
*   `# Роль`: Системный промпт, задающий поведение и роль модели.
*   `# Промпт`: Основная инструкция, которая будет одинакова для всех вопросов в этом файле.
*   `# Настройки`: Правила оценки ответов для этого теста.
    *   `## Сравнение ответа модели текстом: Модель`: Эта настройка означает, что для проверки правильности ответа (сравнения его с эталоном "Колумб") будет вызвана другая, заранее определенная модель-валидатор. Это позволяет оценивать не точное совпадение, а смысловую эквивалентность.
*   `# Тесты`: Набор вопросов и эталонных ответов для проверки.

---

### Пример 3: Файл наборов тестов (`test_suites.md`)

Это главный управляющий файл. Он связывает конфигурации, тесты и модели вместе.

```markdown
# Набор тестов 1
## Описание
Базовая проверка модели Gemini Pro на извлечение метаданных.
## Разрешить выполнение
Да
## Конфигурация
standard
## Модели
google/gemini-pro
## Тесты
get_metadata
```
*   `# Набор тестов 1`: Заголовок, по которому движок находит и запускает набор.
*   `## Разрешить выполнение: Да`: Ключ к запуску. Если установить `Нет`, этот набор будет проигнорирован.
*   `## Конфигурация: standard`: Указание использовать файл `configs/standard.json`.
*   `## Модели: google/gemini-pro`: Какую модель тестируем в этом наборе.
*   `## Тесты: get_metadata`: Указание использовать файл `tests/get_metadata.md`.

## Установка

Если вы пропустили этот шаг в Быстром старте, вот полная инструкция.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL репозитория>
    cd <папка проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Настройте API ключ:**
    *   Создайте файл `.env` в корневой папке проекта.
    *   Добавьте в него ваш ключ API.

## Справочник по форматам файлов

*(Этот раздел содержит детальный синтаксис для каждого конфигурационного файла: `tests/*.md`, `configs/*.json` и `test_suites.md`)*

### Формат файла теста (`tests/*.md`)
*   `# Описание`, `# Роль`, `# Промпт`: Обязательные текстовые секции.
*   `# Настройки`: Опциональная секция с правилами.
    *   `## Допуск при сравнении чисел`: `0.01`
    *   `## Сравнение ... текстом`: `Модель` или `Совпадение <число>`
*   `# Тесты`: Пары `## Вопрос X` и `## Ответ X`.

### Формат файла наборов тестов (`test_suites.md`)
*   `# Набор тестов X`: Заголовок.
*   `## Разрешить выполнение`: `Да` или `Нет`.
*   `## Конфигурация`: Имя JSON-файла из `configs`.
*   `## Модели`: Список моделей через запятую.
*   `## Тесты`: Список `.md` файлов из `tests`.
*   `## Повторы`: (Опционально) Количество запусков.

## Структура проекта

```
/
├─── main.py                  # Главный скрипт для запуска наборов тестов из `test_suites.md`.
├─── tester_engine.py         # Основной движок, выполняющий один полный тестовый прогон.
├─── requirements.txt         # Список зависимостей проекта для установки.
├─── test_suites.md           # Файл для определения наборов тестов, моделей, конфигураций и повторов.
├─── comparison_settings.py   # Класс для хранения и передачи настроек сравнения ответов.
├─── func.py                  # Вспомогательные функции (парсер Markdown, запись в файл).
├─── .env                     # Локальный файл конфигурации с API ключами (необходимо создать).
├─── configs/                 # Папка с JSON-конфигурациями параметров моделей (temperature, max_tokens и т.д.).
│    ├─── standard.json
│    └─── full.json
├─── providers/               # Модули для работы с API поставщиков моделей (например, OpenRouter).
│    └─── open_router.py
├─── report/                  # Модули и итоговые отчеты.
│    ├─── calc_ball.py         # Логика расчета итогового балла.
│    ├─── check.py             # Функции для сверки ответов модели с эталонами.
│    ├─── to_excel.py          # Запись сводных результатов в Excel.
│    └─── report.xlsx          # Итоговый отчет в формате Excel.
├─── result/                  # Папка для сохранения детальных текстовых логов по каждой модели.
├─── tests/                   # Папка с файлами тестов в формате Markdown.
│    ├─── get_metadata.md
│    └─── test_model_check.md
└─── Readme.md                # Документация проекта (этот файл).
```

## Лицензия

Этот проект распространяется под лицензией MIT.