# Система для тестирования и оценки LLM моделей

Это приложение представляет собой гибкий и мощный инструмент для проведения тестирования и оценки производительности различных больших языковых моделей (LLM). 

С его помощью вы можете автоматизировать проверку качества ответов моделей, сравнить их между собой по скорости, точности и стоимости, а также найти оптимальные параметры для решения ваших задач.

## Быстрый старт

Этот раздел поможет вам запустить проект и получить первые результаты за 3 шага.

1.  **Настройте API-ключ.** Создайте в корне проекта файл `.env` и добавьте в него ваш ключ от [OpenRouter](https://openrouter.ai/keys):
    ```
    OPENROUTER_API_KEY="ваш_ключ_здесь"
    ```

2.  **Установите зависимости.** Выполните в терминале:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Запустите тесты.** В проекте уже есть готовый демонстрационный набор. Просто запустите главный скрипт:
    ```bash
    python3 main.py
    ```

Готово! Вы увидите в консоли ход выполнения тестов.

## Что вы получите: Форматы отчетов

После выполнения скрипта вы получите результаты в трех форматах:

1.  **Вывод в консоли:** Наглядные таблицы с прогрессом и итоговой статистикой по каждому тесту.
2.  **Детальный лог (`result/`):** В эту папку для каждой модели создается текстовый файл с полной историей диалога: вопросы, эталонные ответы, ответы модели и вердикт (ВЕРНО/ОШИБКА).
3.  **Сводный отчет (`report/report.xlsx`):** В эту Excel-таблицу после каждого запуска добавляется строка с ключевыми метриками: модель, тест, медианное время ответа, процент правильных ответов, итоговый балл и стоимость.

## Как это работает: Руководство по форматам

Система имеет гибкую структуру, состоящую из трех ключевых типов файлов. Они работают вместе, чтобы обеспечить полный цикл тестирования. Ниже описан каждый из них в порядке их использования.

---

### 1. Файл наборов тестов (`test_suites.md`)

Это главный управляющий файл. Он определяет, *какие* модели, *на каких* тестах и с *какими* конфигурациями будут запущены.

```markdown
# Набор тестов 1
## Описание
Базовая проверка модели Gemini Pro на извлечение метаданных.
## Разрешить выполнение
Да
## Конфигурация
standard
## Модели
google/gemini-pro, google/gemini-flash
## Тесты
get_metadata, test_model_check
```

*   `# Набор тестов X`: Заголовок, по которому движок находит и запускает набор.
*   `## Разрешить выполнение: Да`: Ключ к запуску. Если установить `Нет`, этот набор будет проигнорирован.
*   `## Конфигурация: standard`: Указание использовать файл `configs/standard.json`.
*   `## Модели`: Какие модели тестируем. **Можно указать несколько через запятую**, и тогда каждая из них пройдет все указанные тесты.
*   `## Тесты`: Какие тесты используем. **Можно указать несколько через запятую**, чтобы запустить их для каждой из указанных моделей.

> В примере выше `google/gemini-pro` и `google/gemini-flash` будут последовательно запущены на тестах `get_metadata` и `test_model_check`.

---

### 2. Файл теста (`tests/*.md`)

Это сценарий проверки, который содержит промпты, вопросы, эталонные ответы и, что самое важное, **правила оценки**.

Ниже приведен пример файла `tests/test_model_check.md`, демонстрирующий использование разных правил в секции `#Настройки`.

```markdown
# Описание
Проверка проверки ответа модели другой моделью.

# Роль
Ты — консультант.

# Промпт
Ответь на вопрос коротко и только по делу.
Ответь текстом без кавычек.

# Настройки
## Допуск при сравнении чисел
0.01
## Сравнение строк в списке
Совпадение 100
## Сравнение строк в словаре
Совпадение 75
## Сравнение ответа модели текстом
Модель

# Тесты
## Вопрос 1
кто открыл Америку?
## Ответ 1
Колумб
```

*   `# Описание`, `# Роль`, `# Промпт`, `# Тесты`: Стандартные секции.
*   `# Настройки`: В этом примере секция демонстрирует сразу несколько правил:
    *   `## Допуск при сравнении чисел: 0.01`: Правило для сравнения чисел (в данном примере не используется, но показывает синтаксис).
    *   `## Сравнение строк в списке: Совпадение 100`: Правило для сравнения строк в списках JSON (в данном примере не используется).
    *   `## Сравнение строк в словаре: Совпадение 75`: Правило для сравнения строк в словарях JSON (в данном примере не используется).
    *   `## Сравнение ответа модели текстом: Модель`: Основное правило для этого теста. Ответ модели на вопрос будет считаться верным, если другая модель-валидатор подтвердит его смысловую эквивалентность с эталоном ("Колумб").

#### Гибкая оценка ответов: Секция `#Настройки`

Эта секция позволяет гибко определять, что считать "правильным" ответом.

**1. Допуск при сравнении чисел**

*   **Синтаксис:** `## Допуск при сравнении чисел: 0.01`
*   **Как работает:** При сравнении числовых значений (внутри JSON) используется формула `abs(эталонное_число - число_модели) <= допуск`.
*   **Для чего нужно:** Необходимо для тестов, где ответы содержат числа с плавающей точкой. Например, если эталон `3.14`, а модель вернула `3.141`, с допуском `0.01` ответ будет засчитан как верный.

**2. Сравнение строк (для текста, списков и словарей)**

Для сравнения текстовых данных существует два режима:

*   **Режим 1: Сравнение по схожести**
    *   **Синтаксис:** `## Сравнение ответа модели текстом: Совпадение 85` (также для `...строк в списке` и `...в словаре`).
    *   **Как работает:** Используется алгоритм нечеткого сравнения, который вычисляет "процент похожести" двух строк. Ответ считается верным, если процент схожести не ниже заданного порога.
    *   **Для чего нужно:** Идеально, когда формулировка не важна, но ключевые слова должны присутствовать.

*   **Режим 2: Оценка с помощью Модели**
    *   **Синтаксис:** `## Сравнение ответа модели текстом: Модель` (также для `...строк в списке` и `...в словаре`).
    *   **Как работает:** Система делает запрос к другой "модели-валидатору", которая оценивает, являются ли эталонный ответ и ответ модели семантически эквивалентными.
    *   **Для чего нужно:** Самый продвинутый режим для оценки ответов на открытые вопросы, где важен смысл, а не дословное совпадение.

**Контекст применения правил**

*   `Сравнение ответа модели текстом`: Для случаев, когда ответ — это простой текст (не JSON).
*   `Сравнение строк в списке`: Для строковых элементов внутри списков в JSON.
*   `Сравнение строк в словаре`: Для строковых значений в словарях в JSON.

---

### 3. Файл конфигурации (`configs/*.json`)

В этом файле задаются параметры для API модели, такие как "креативность" (`temperature`) и максимальная длина ответа. В проекте уже есть готовые `standard.json` и `full.json`.

```json
{
  "param": {
    "temperature": 0.5,
    "max_tokens": 2048
  },
  "response_format": {
    "type": "json_object"
  },
  "extra_body": null
}
```
*   `param`: Основные параметры, которые передаются в API модели.
    *   `temperature`: Контролирует случайность ответов. `0.5` — сбалансированный вариант.
    *   `max_tokens`: Максимальное количество токенов в ответе.
*   `response_format`: Указывает модели, что ответ должен быть в формате JSON.
*   `extra_body`: Дополнительные, реже используемые параметры.

## Установка

Если вы пропустили этот шаг в Быстром старте, вот полная инструкция.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL репозитория>
    cd <папка проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Настройте API ключ:**
    *   Создайте файл `.env` в корневой папке проекта.
    *   Добавьте в него ваш ключ API.

## Справочник по форматам файлов

*(Этот раздел содержит детальный синтаксис для каждого конфигурационного файла: `tests/*.md`, `configs/*.json` и `test_suites.md`)*

### Формат файла теста (`tests/*.md`)
*   `# Описание`, `# Роль`, `# Промпт`: Обязательные текстовые секции.
*   `# Настройки`: Опциональная секция с правилами.
    *   `## Допуск при сравнении чисел`: `0.01`
    *   `## Сравнение ... текстом`: `Модель` или `Совпадение <число>`
*   `# Тесты`: Пары `## Вопрос X` и `## Ответ X`.

### Формат файла наборов тестов (`test_suites.md`)
*   `# Набор тестов X`: Заголовок.
*   `## Разрешить выполнение`: `Да` или `Нет`.
*   `## Конфигурация`: Имя JSON-файла из `configs`.
*   `## Модели`: Список моделей через запятую.
*   `## Тесты`: Список `.md` файлов из `tests`.
*   `## Повторы`: (Опционально) Количество запусков.

## Структура проекта

```
/
├─── main.py                  # Главный скрипт для запуска наборов тестов из `test_suites.md`.
├─── tester_engine.py         # Основной движок, выполняющий один полный тестовый прогон.
├─── requirements.txt         # Список зависимостей проекта для установки.
├─── test_suites.md           # Файл для определения наборов тестов, моделей, конфигураций и повторов.
├─── comparison_settings.py   # Класс для хранения и передачи настроек сравнения ответов.
├─── func.py                  # Вспомогательные функции (парсер Markdown, запись в файл).
├─── .env                     # Локальный файл конфигурации с API ключами (необходимо создать).
├─── configs/                 # Папка с JSON-конфигурациями параметров моделей (temperature, max_tokens и т.д.).
│    ├─── standard.json
│    └─── full.json
├─── providers/               # Модули для работы с API поставщиков моделей (например, OpenRouter).
│    └─── open_router.py
├─── report/                  # Модули и итоговые отчеты.
│    ├─── calc_ball.py         # Логика расчета итогового балла.
│    ├─── check.py             # Функции для сверки ответов модели с эталонами.
│    ├─── to_excel.py          # Запись сводных результатов в Excel.
│    └─── report.xlsx          # Итоговый отчет в формате Excel.
├─── result/                  # Папка для сохранения детальных текстовых логов по каждой модели.
├─── tests/                   # Папка с файлами тестов в формате Markdown.
│    ├─── get_metadata.md
│    └─── test_model_check.md
└─── Readme.md                # Документация проекта (этот файл).
```

## Лицензия

Этот проект распространяется под лицензией MIT.