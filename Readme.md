# Система для тестирования и оценки LLM моделей

Это приложение представляет собой гибкий и мощный инструмент для проведения тестирования и оценки производительности различных больших языковых моделей (LLM). 

С его помощью вы можете автоматизировать проверку качества ответов моделей, сравнить их между собой по скорости, точности и стоимости, а также найти оптимальные параметры для решения ваших задач.

## Быстрый старт

Этот раздел поможет вам запустить проект и получить первые результаты за 3 шага.

1.  **Настройте API-ключ.** Создайте в корне проекта файл `.env` и добавьте в него ваш ключ от [OpenRouter](https://openrouter.ai/keys):
    ```
    OPENROUTER_API_KEY="ваш_ключ_здесь"
    ```

2.  **Установите зависимости.** Выполните в терминале:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Запустите тесты.** В проекте уже есть готовый демонстрационный набор. Просто запустите главный скрипт:
    ```bash
    python3 main.py
    ```

Готово! Вы увидите в консоли ход выполнения тестов.

## Что вы получите: Форматы отчетов

После выполнения скрипта вы получите результаты в трех форматах, каждый для своей цели:

### 1. Вывод в консоли

**Назначение:** Быстрое отслеживание прогресса выполнения тестов в реальном времени.

**Содержание:** Показывает детальный прогресс по каждому вопросу, итоговую сводку для каждого запуска и **общую стоимость** в конце.

```
--- Тестирование. Модель: mistralai/codestral-2508, Тест: test_model_check.md ---

--- Запуск (Повтор 1/2) ---
Вопрос 1 - ВЕРНО  (Время: 0.59)
Вопрос 2 - ВЕРНО  (Время: 0.57)
Вопрос 3 - ВЕРНО  (Время: 0.31)
Вопрос 4 - ВЕРНО  (Время: 0.77)

Итоги по тесту 'test_model_check' для модели 'mistralai/codestral-2508':
Медианное время выполнения - 0.58
Процент правильных ответов - 100
Баллов за тест - 94
Цена - 0.0001038

--- Запуск (Повтор 2/2) ---
... (аналогичный вывод для второго повтора) ...
--- Все повторы для теста test_model_check завершены ---

... (далее следует вывод для других моделей) ...

==================== Все наборы тестов обработаны ====================

ОБЩАЯ СВОДКА ПО СТОИМОСТИ:
  - Общая стоимость всех тестов: $0.00045732
  - Разбивка по моделям:
    - mistralai/codestral-2508: $0.0002094
    - mistralai/ministral-3b: $0.00001732
    - mistralai/mistral-small: $0.0002306
```
*   `Итоги по тесту`: Промежуточная статистика по одному конкретному запуску (одна модель, один тест, один повтор).
*   `ОБЩАЯ СВОДКА ПО СТОИМОСТИ`: Финальный блок, который появляется после обработки всех наборов тестов. Он показывает общую сумму, потраченную на все запуски, и дает разбивку по каждой использованной модели.

### 2. Детальный лог (`result/<имя_модели>.txt`)

**Назначение:** Глубокий анализ и отладка ответов конкретной модели.

**Содержание:** Записывает исчерпывающую информацию по каждому запуску, включая полные ответы модели и разбивку по токенам. Каждый запуск отделяется набором символов `/`.

```
+----------+-------------------------------------------------‍+
| Дата     | 31.08.2025 12:15:36                             |
| Модель   | mistralai/mistral-small                         |
| Ввод     | 0.20$ за 1М                                     |
| Вывод    | 0.60$ за 1М                                     |
| Тест     | test_model_check                                |
| Описание | Проверка проверки ответа модели другой моделью. |
+----------+-------------------------------------------------‍+
Вопрос 1:
Который час?
Ответ модели:
Текущее время — 14:30.
Правильный ответ:
любое дневное время правильно
+------------------+-----------+
| Проверка         | ВЕРНО     |
| Токенов Ввод     | 60        |
| Токенов Вывод    | 14        |
| Цена запроса     | 0.0000204 |
| Время выполнения | 0.45      |
+------------------+-----------+
... (и так для каждого вопроса) ...

ИТОГ:
+----------------------------+--------+
| Всего вопросов             | 4      |
| Правильных ответов         | 4      |
| Процент правильных ответов | 100    |
| Баллов за тест             | 97     |
| Токенов Ввод               | 248    |
| Токенов Вывод              | 84     |
| Цена                       | 0.0001 |
| Время выполнения           | 4.17   |
+----------------------------+--------+
```
*   **Верхняя таблица**: Содержит метаданные запуска: время, имя модели, стоимость токенов на момент теста и название теста.
*   **Блок вопроса**: Для каждого вопроса в тесте создается свой блок.
    *   `Вопрос`: Текст вопроса, отправленного модели.
    *   `Ответ модели`: **Полный, без сокращений** ответ, который дала модель.
    *   `Правильный ответ`: Эталонный ответ из файла теста.
*   **Таблица проверки вопроса**: Статистика по конкретному вопросу.
    *   `Проверка`: Вердикт (ВЕРНО/ОШИБКА).
    *   `Токенов Ввод/Вывод`: Количество токенов, потраченных на промпт и на генерацию ответа.
    *   `Цена запроса`: Стоимость конкретно этого вопроса.
    *   `Время выполнения`: Время ответа на конкретный вопрос.
*   `ИТОГ`: Финальная таблица со сводной статистикой по всему запуску, включая общее количество токенов.

### 3. Сводный отчет (`report/report.xlsx`)

**Назначение:** Сравнение результатов между моделями и тестами, построение графиков.

**Содержание:** В эту Excel-таблицу после каждого запуска добавляется **одна строка** с ключевыми метриками. Идеально подходит для анализа производительности разных моделей в динамике.

| Дата/время          | Модель                    | Тест               | Задержка | % верно | Балл | Цена      |
| ------------------- | ------------------------- | ------------------ | -------- | ------- | ---- | --------- |
| 31.08.2025 12:15:23 | mistralai/codestral-2508  | test_model_check   | 0,46     | 100,00  | 100  | 0,0001056 |
| 31.08.2025 12:15:31 | mistralai/mistral-small   | test_model_check   | 0,53     | 75,00   | 73   | 0,0001306 |


## Как это работает: Руководство по форматам

Система имеет гибкую структуру, состоящую из трех ключевых типов файлов. Они работают вместе, чтобы обеспечить полный цикл тестирования. Ниже описан каждый из них в порядке их использования.

---

### 1. Файл наборов тестов (`test_suites.md`)

Это главный управляющий файл. Он определяет, *какие* модели, *на каких* тестах и с *какими* конфигурациями будут запущены.

```markdown
# Набор тестов 1
## Описание
Базовая проверка модели Gemini Pro на извлечение метаданных.
## Разрешить выполнение
Да
## Конфигурация
standard
## Модели
google/gemini-pro, google/gemini-flash
## Тесты
get_metadata, test_model_check
```

*   `# Набор тестов X`: Заголовок, по которому движок находит и запускает набор.
*   `## Разрешить выполнение: Да`: Ключ к запуску. Если установить `Нет`, этот набор будет проигнорирован.
*   `## Конфигурация: standard`: Указание использовать файл `configs/standard.json`.
*   `## Модели`: Какие модели тестируем. **Можно указать несколько через запятую**, и тогда каждая из них пройдет все указанные тесты.
*   `## Тесты`: Какие тесты используем. **Можно указать несколько через запятую**, чтобы запустить их для каждой из указанных моделей.

> В примере выше `google/gemini-pro` и `google/gemini-flash` будут последовательно запущены на тестах `get_metadata` и `test_model_check`.

---

### 2. Файл теста (`tests/*.md`)

Это сценарий проверки, который содержит промпты, вопросы, эталонные ответы и, что самое важное, **правила оценки**.

Ниже приведен пример файла `tests/test_model_check.md`, демонстрирующий использование разных правил в секции `#Настройки`.

```markdown
# Описание
Проверка проверки ответа модели другой моделью.

# Роль
Ты — консультант.

# Промпт
Ответь на вопрос коротко и только по делу.
Ответь текстом без кавычек.

# Настройки
## Допуск при сравнении чисел
0.01
## Сравнение строк в списке
Совпадение 100
## Сравнение строк в словаре
Совпадение 75
## Сравнение ответа модели текстом
Модель

# Тесты
## Вопрос 1
кто открыл Америку?
## Ответ 1
Колумб
```

*   `# Описание`, `# Роль`, `# Промпт`, `# Тесты`: Стандартные секции.
*   `# Настройки`: В этом примере секция демонстрирует сразу несколько правил:
    *   `## Допуск при сравнении чисел: 0.01`: Правило для сравнения чисел (в данном примере не используется, но показывает синтаксис).
    *   `## Сравнение строк в списке: Совпадение 100`: Правило для сравнения строк в списках JSON (в данном примере не используется).
    *   `## Сравнение строк в словаре: Совпадение 75`: Правило для сравнения строк в словарях JSON (в данном примере не используется).
    *   `## Сравнение ответа модели текстом: Модель`: Основное правило для этого теста. Ответ модели на вопрос будет считаться верным, если другая модель-валидатор подтвердит его смысловую эквивалентность с эталоном ("Колумб").

#### Гибкая оценка ответов: Секция `#Настройки`

Эта секция позволяет гибко определять, что считать "правильным" ответом.

**1. Допуск при сравнении чисел**

*   **Синтаксис:** `## Допуск при сравнении чисел: 0.01`
*   **Как работает:** При сравнении числовых значений (внутри JSON) используется формула `abs(эталонное_число - число_модели) <= допуск`.
*   **Для чего нужно:** Необходимо для тестов, где ответы содержат числа с плавающей точкой. Например, если эталон `3.14`, а модель вернула `3.141`, с допуском `0.01` ответ будет засчитан как верный.

**2. Сравнение строк (для текста, списков и словарей)**

Для сравнения текстовых данных существует два режима:

*   **Режим 1: Сравнение по схожести**
    *   **Синтаксис:** `## Сравнение ответа модели текстом: Совпадение 85` (также для `...строк в списке` и `...в словаре`).
    *   **Как работает:** Используется алгоритм нечеткого сравнения, который вычисляет "процент похожести" двух строк. Ответ считается верным, если процент схожести не ниже заданного порога.
    *   **Для чего нужно:** Идеально, когда формулировка не важна, но ключевые слова должны присутствовать.

*   **Режим 2: Оценка с помощью Модели**
    *   **Синтаксис:** `## Сравнение ответа модели текстом: Модель` (также для `...строк в списке` и `...в словаре`).
    *   **Как работает:** Система делает запрос к другой "модели-валидатору", которая оценивает, являются ли эталонный ответ и ответ модели семантически эквивалентными.
    *   **Для чего нужно:** Самый продвинутый режим для оценки ответов на открытые вопросы, где важен смысл, а не дословное совпадение.

**Контекст применения правил**

*   `Сравнение ответа модели текстом`: Для случаев, когда ответ — это простой текст (не JSON).
*   `Сравнение строк в списке`: Для строковых элементов внутри списков в JSON.
*   `Сравнение строк в словаре`: Для строковых значений в словарях в JSON.

---

### 3. Файл конфигурации (`configs/*.json`)

В этом файле задаются параметры для API модели, такие как "креативность" (`temperature`) и максимальная длина ответа. В проекте уже есть готовые `standard.json` и `full.json`.

```json
{
  "param": {
    "temperature": 0.5,
    "max_tokens": 2048
  },
  "response_format": {
    "type": "json_object"
  },
  "extra_body": null
}
```
*   `param`: Основные параметры, которые передаются в API модели.
    *   `temperature`: Контролирует случайность ответов. `0.5` — сбалансированный вариант.
    *   `max_tokens`: Максимальное количество токенов в ответе.
*   `response_format`: Указывает модели, что ответ должен быть в формате JSON.
*   `extra_body`: Дополнительные, реже используемые параметры.

## Установка

Если вы пропустили этот шаг в Быстром старте, вот полная инструкция.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL репозитория>
    cd <папка проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Настройте API ключ:**
    *   Создайте файл `.env` в корневой папке проекта.
    *   Добавьте в него ваш ключ API.

## Справочник по форматам файлов

*(Этот раздел содержит детальный синтаксис для каждого конфигурационного файла: `tests/*.md`, `configs/*.json` и `test_suites.md`)*

### Формат файла теста (`tests/*.md`)
*   `# Описание`, `# Роль`, `# Промпт`: Обязательные текстовые секции.
*   `# Настройки`: Опциональная секция с правилами.
    *   `## Допуск при сравнении чисел`: `0.01`
    *   `## Сравнение ... текстом`: `Модель` или `Совпадение <число>`
*   `# Тесты`: Пары `## Вопрос X` и `## Ответ X`.

### Формат файла наборов тестов (`test_suites.md`)
*   `# Набор тестов X`: Заголовок.
*   `## Разрешить выполнение`: `Да` или `Нет`.
*   `## Конфигурация`: Имя JSON-файла из `configs`.
*   `## Модели`: Список моделей через запятую.
*   `## Тесты`: Список `.md` файлов из `tests`.
*   `## Повторы`: (Опционально) Количество запусков.

## Структура проекта

```
/
├─── main.py                  # Главный скрипт для запуска наборов тестов из `test_suites.md`.
├─── tester_engine.py         # Основной движок, выполняющий один полный тестовый прогон.
├─── requirements.txt         # Список зависимостей проекта для установки.
├─── test_suites.md           # Файл для определения наборов тестов, моделей, конфигураций и повторов.
├─── comparison_settings.py   # Класс для хранения и передачи настроек сравнения ответов.
├─── func.py                  # Вспомогательные функции (парсер Markdown, запись в файл).
├─── .env                     # Локальный файл конфигурации с API ключами (необходимо создать).
├─── configs/                 # Папка с JSON-конфигурациями параметров моделей (temperature, max_tokens и т.д.).
│    ├─── standard.json
│    └─── full.json
├─── providers/               # Модули для работы с API поставщиков моделей (например, OpenRouter).
│    └─── open_router.py
├─── report/                  # Модули и итоговые отчеты.
│    ├─── calc_ball.py         # Логика расчета итогового балла.
│    ├─── check.py             # Функции для сверки ответов модели с эталонами.
│    ├─── to_excel.py          # Запись сводных результатов в Excel.
│    └─── report.xlsx          # Итоговый отчет в формате Excel.
├─── result/                  # Папка для сохранения детальных текстовых логов по каждой модели.
├─── tests/                   # Папка с файлами тестов в формате Markdown.
│    ├─── get_metadata.md
│    └─── test_model_check.md
└─── Readme.md                # Документация проекта (этот файл).
```

## Лицензия

Этот проект распространяется под лицензией MIT.