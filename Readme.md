# Система для тестирования и оценки LLM моделей

Это приложение представляет собой гибкий и мощный инструмент для проведения тестирования и оценки производительности различных больших языковых моделей (LLM). 

С его помощью вы можете автоматизировать проверку качества ответов моделей, сравнить их между собой по скорости, точности и стоимости, а также найти оптимальные параметры для решения ваших задач.

## Быстрый старт

Этот раздел поможет вам запустить проект и получить первые результаты за 3 шага.

1.  **Настройте API-ключ.** Создайте в корне проекта файл `.env` и добавьте в него ваш ключ от [OpenRouter](https://openrouter.ai/keys):
    ```
    OPENROUTER_API_KEY="ваш_ключ_здесь"
    ```

2.  **Установите зависимости.** Выполните в терминале:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Запустите тесты.** В проекте уже есть готовый демонстрационный набор. Просто запустите главный скрипт:
    ```bash
    python3 main.py
    ```

Готово! Вы увидите в консоли ход выполнения тестов.

## Что вы получите: Форматы отчетов

После выполнения скрипта вы получите результаты в трех форматах:

1.  **Вывод в консоли:** Наглядные таблицы с прогрессом и итоговой статистикой по каждому тесту.
2.  **Детальный лог (`result/`):** В эту папку для каждой модели создается текстовый файл с полной историей диалога: вопросы, эталонные ответы, ответы модели и вердикт (ВЕРНО/ОШИБКА).
3.  **Сводный отчет (`report/report.xlsx`):** В эту Excel-таблицу после каждого запуска добавляется строка с ключевыми метриками: модель, тест, медианное время ответа, процент правильных ответов, итоговый балл и стоимость.

## Как это работает: Углубленное руководство

Система имеет гибкую структуру, которую вы можете настраивать для решения своих задач.

### Ключевые концепции

1.  **Набор тестов (`Test Suite`)**: Верхнеуровневая сущность в `test_suites.md`. Определяет, *какие* модели тестировать, *на каких* тестах и с *какими* параметрами.
2.  **Тест (`Test`)**: Сценарий в `.md` файле в папке `tests/`. Содержит промпты, вопросы, ответы и **настройки для оценки**.
3.  **Конфигурация (`Config`)**: JSON-файл в `configs/`. Определяет технические параметры запроса к API (`temperature`, `max_tokens` и т.д.).

### Процесс создания собственного теста

#### Шаг 1: Создание файла теста (`tests/`)

Создайте `.md` файл (например, `my_text_test.md`). В нем, помимо основных секций (`# Описание`, `# Роль`, `# Промпт`, `# Тесты`), вы можете добавить секцию `# Настройки` для тонкой настройки правил сравнения ответов (например, задать числовой допуск или указать, что для оценки ответа нужно использовать другую модель).

#### Шаг 2: Создание файла конфигурации (`configs/`)

Создайте `.json` файл (например, `creative.json`) с параметрами для API.

#### Шаг 3: Определение набора тестов (`test_suites.md`)

Добавьте в `test_suites.md` новый блок `# Набор тестов X`, который свяжет ваши тесты, модели и конфигурации. Укажите `Разрешить выполнение: Да`.

Теперь при запуске `python3 main.py` будет выполнен и ваш новый набор тестов.

## Установка

Если вы пропустили этот шаг в Быстром старте, вот полная инструкция.

1.  **Клонируйте репозиторий:**
    ```bash
    git clone <URL репозитория>
    cd <папка проекта>
    ```

2.  **Создайте и активируйте виртуальное окружение (рекомендуется):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Настройте API ключ:**
    *   Создайте файл `.env` в корневой папке проекта.
    *   Добавьте в него ваш ключ API.

## Справочник по форматам файлов

*(Этот раздел содержит детальный синтаксис для каждого конфигурационного файла: `tests/*.md`, `configs/*.json` и `test_suites.md`)*

### Формат файла теста (`tests/*.md`)
*   `# Описание`, `# Роль`, `# Промпт`: Обязательные текстовые секции.
*   `# Настройки`: Опциональная секция с правилами.
    *   `## Допуск при сравнении чисел`: `0.01`
    *   `## Сравнение ... текстом`: `Модель` или `Совпадение <число>`
*   `# Тесты`: Пары `## Вопрос X` и `## Ответ X`.

### Формат файла наборов тестов (`test_suites.md`)
*   `# Набор тестов X`: Заголовок.
*   `## Разрешить выполнение`: `Да` или `Нет`.
*   `## Конфигурация`: Имя JSON-файла из `configs`.
*   `## Модели`: Список моделей через запятую.
*   `## Тесты`: Список `.md` файлов из `tests`.
*   `## Повторы`: (Опционально) Количество запусков.

## Структура проекта

```
/
├─── main.py                  # Главный скрипт для запуска наборов тестов из `test_suites.md`.
├─── tester_engine.py         # Основной движок, выполняющий один полный тестовый прогон.
├─── requirements.txt         # Список зависимостей проекта для установки.
├─── test_suites.md           # Файл для определения наборов тестов, моделей, конфигураций и повторов.
├─── comparison_settings.py   # Класс для хранения и передачи настроек сравнения ответов.
├─── func.py                  # Вспомогательные функции (парсер Markdown, запись в файл).
├─── .env                     # Локальный файл конфигурации с API ключами (необходимо создать).
├─── configs/                 # Папка с JSON-конфигурациями параметров моделей (temperature, max_tokens и т.д.).
│    ├─── standard.json
│    └─── full.json
├─── providers/               # Модули для работы с API поставщиков моделей (например, OpenRouter).
│    └─── open_router.py
├─── report/                  # Модули и итоговые отчеты.
│    ├─── calc_ball.py         # Логика расчета итогового балла.
│    ├─── check.py             # Функции для сверки ответов модели с эталонами.
│    ├─── to_excel.py          # Запись сводных результатов в Excel.
│    └─── report.xlsx          # Итоговый отчет в формате Excel.
├─── result/                  # Папка для сохранения детальных текстовых логов по каждой модели.
├─── tests/                   # Папка с файлами тестов в формате Markdown.
│    ├─── get_metadata.md
│    └─── test_model_check.md
└─── Readme.md                # Документация проекта (этот файл).
```

## Лицензия

Этот проект распространяется под лицензией MIT.